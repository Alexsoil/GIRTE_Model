from os.path import join, isdir
from os import listdir

from tqdm import tqdm

from Preprocess.Tok_Document import TokDocument
from Preprocess.Collection import Collection

class TokCollection(Collection):
    r"""
        Based on the existing Collection class, this subclass adds functionality
        needed for organizing data into Collections based not on the plain terms
        of the documents, but the tokens generated by the BERT. 
        Intended to be used with the GIRTE class for Information Retrieval tasks.

        Args:
            path (`str`):
                The path of the collection file on the disk.
            docs (`list`, defaults to `None`):
                Documents (`TokDocument` instances) to be loaded directly.
            name (`str`, defaults to `''`):
                A given name for the collection.
            bert (`str`, {`'base'` or `'large'`}, defaults to `'base'`):
                Whether to use *bert-base-uncased* or *bert-large-uncased*.
            stopwords (`bool`, defaults to `False`):
                Whether or not to filter stopwords out of the document prior
                to processing. Stopwords defined by nltk.corpus.stopwords('english).
    """

    def __init__(self, path, docs=None, name='', bert='base', stopwords=False):
        super().__init__(path, docs, name)
        self._bert = 'large' if bert == 'large' else 'base'
        if bert != 'base' and bert != 'large':
            print('Warning! BERT type not defined as "base" or "large", defaulting to "base".')
        self._stopwords = stopwords

    def create_collection(self):
        # Create TokDocument objects from path and load them into collection
        print(f'Creating token-based collection. BERT={self._bert} Stopwords={self._stopwords}')
        self.num_docs = 0
        if not self.docs:
            filenames = [join(self.path, id) for id in listdir(self.path)]
            max_id = max([int(id) for id in listdir(self.path)])
            self.num_docs = int(max_id)
            for fn in tqdm(filenames):
                if not isdir(fn):
                    self.docs.append(TokDocument(fn, self._bert, self._stopwords))
            self.inverted_index = self.create_inverted_index()
    
    def create_inverted_index(self):
        # Create inverted index for collection, using tokens instead of terms
        inv_index = {}
        token_id = 0
        error_counter = 0
        try:
            for doc in self.docs:
                print(f'Processing doc {doc.doc_id}')
                for token, occurances in doc.token_frequency.items():
                    if token not in inv_index:
                        inv_index[token] = {
                            'id': token_id,
                            'total_occurances': occurances,
                            'posting_list': [[doc.doc_id, occurances]],
                            'token': token
                        }
                        token_id += 1
                    elif token in inv_index:
                        inv_index[token]['total_occurances'] += occurances
                        inv_index[token]['posting_list'] += [[doc.doc_id, occurances]]
        except KeyError as Err:
            print(Err)
            error_counter += 1
            print(f'Keys not found {error_counter}')
        return inv_index