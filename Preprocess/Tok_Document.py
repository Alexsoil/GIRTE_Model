from Preprocess.Document import Document
from re import findall
from utilities.document_utls import calculate_tf
from transformers import BertTokenizer, BertModel
import torch
import os
from pickle import dump

class TokDocument(Document):
    """
        Based on the existing Document class, this subclass adds functionality
        needed for tokenizing the text using the BertTokenizer and calculating
        token frequency. Necessary for implementing the token based GSB Model.
        
        A TokDocument object consists of:
        - path: str - The path of the document file on disk.
        - doc_id: int - Unique ID number for the document.
        - terms: [str] - List of terms included in the document.
        - text: str - The complete text of the document.
        - tokens: [str] - List of tokens generated by BertTokenizer.
        - token_frequency {str: int} - Dictionary of tokens and the respective number they appear in the document.
    """
    def __init__(self, path=''):
        try:
            self.path = path
        except FileNotFoundError:
            raise FileNotFoundError
        try:
            self.doc_id = int(findall(r'\d+', self.path)[0])
        except IndexError:
            self.doc_id = 696969
        self.terms = self.read_document()
        self.text = ' '.join(self.terms)
        # self.tokens = [] <- If needed can be gotten from doc_tokenize func
        self.token_frequency = {}
        # save aggregate tensors on disk
        self.tensor_path = 'C:/picklejar/collections/CF/tensors'
        os.makedirs(self.tensor_path, exist_ok=True)
        with open(os.path.join(self.tensor_path, str(self.doc_id)), 'wb') as picklefile:
            dump(self.doc_tokenize(), picklefile)
        
    
    def __str__(self):
        return f'ID: {self.doc_id}\nTerms: {self.terms}\nTokens: {self.tokens}'
    
    def doc_tokenize(self):
        # Initialize and run BERT and generate tokens and embeddings
        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
        model = BertModel.from_pretrained('bert-base-uncased')
        encoding = tokenizer.__call__(
            self.terms,
            padding=True,
            truncation=True,
            add_special_tokens=False,
            is_split_into_words=True,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids']
        attention_mask = encoding['attention_mask']
        # Tokens
        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask)
            word_embeddings = outputs.last_hidden_state
        # Embeddings
        tensors = word_embeddings[0]
        tensor_list = []
        for i in range(len(tensors)):
            tensor_list.append(tensors[i])
        # Token frequency calculated here to save on file size
        self.token_frequency = calculate_tf(tokens)
        # Aggregate the tensors of every unique token and save as dictionary {token: tensor}
        aggregate_tensors = {}
        for token, tensor in zip(tokens, tensor_list):
            if token not in aggregate_tensors:
                aggregate_tensors[token] = tensor
            elif token in aggregate_tensors:
                current_tensor = aggregate_tensors[token]
                new_tensor = torch.mean(torch.stack((current_tensor, tensor)), dim=0)
                aggregate_tensors[token] = new_tensor
        return aggregate_tensors
